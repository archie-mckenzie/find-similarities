# find_matching.py
# Python script for finding similar rare words across two corpuses
# Â© 2023 Archie McKenzie, MIT License

# For each pair, tokenizes using tiktoken
# Creates a frequency table of tokens used in each corpus
# Then, rearranges sentences by the rarity of their matching words

# ------- IMPORTS ------- #

# Import natural language toolkit
import nltk
import ssl
try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context
nltk.download('punkt')

# Import tiktoken
import tiktoken
enc = tiktoken.get_encoding("cl100k_base")

# Import counter for the frequency tables
from collections import Counter

# Import json for exporting output
import json

# ------- EDITABLE VARIABLES ------- #
input_path_1 = 'input/dial_of_princes.txt'
input_path_2 = 'input/wits_misery.txt'
output_path = 'output/matching.json' # sorted

# ------- OTHER FUNCTIONS ------- #

# Takes in a passage, tokenizes it, and returns a frequency table for each token
def make_token_freq_table(text):
    tokenized_text = enc.encode(text)
    return len(tokenized_text), Counter(tokenized_text) # construct frequency table using Counter

# Split any given string into sentences
# Return an array of strings
def split_into_sentences(text):
    sentences = nltk.sent_tokenize(text)
    return sentences

# Tokenize each of the strings in a given array of strings
# Return an array of integers (tokens)
def tokenize_sentences(sentences):
    tokenized_sentences = []
    for sentence in sentences:
        tokenized_sentences.append(enc.encode(sentence))
    return tokenized_sentences

# sort a list of pair jsons by the similarity index
def sort_by_matching_score(pairs):
    return sorted(pairs, key=lambda x: x['matchingWordRarity'], reverse=True)
    
# ------- PROCESS .TXT FILES ------- # 

# Read the .txt files
input_1 = open(input_path_1, 'r').read()
input_2 = open(input_path_2, 'r').read()

# Construct frequency table of tokens
num_tokens_1, freq_table_1 = make_token_freq_table(input_1)
num_tokens_2, freq_table_2 = make_token_freq_table(input_2)

# Split into sentences
sentences_1 = split_into_sentences(input_1)
sentences_2 = split_into_sentences(input_2)

# Tokenize sentences using tiktoken
tokenized_sentences_1 = tokenize_sentences(sentences_1)
tokenized_sentences_2 = tokenize_sentences(sentences_2)

pairs = []

# Give each sentence pair a score based on the rarity of their matching words
for i, alpha in enumerate(tokenized_sentences_1):
    for j, beta in enumerate(tokenized_sentences_2):
        matching_score = 0
        for token_a in alpha:
            for token_b in beta:
                if (token_a == token_b):
                    incidence_1 = freq_table_1[token_a] if (freq_table_1[token_a] != 0) else 1
                    incidence_2 = freq_table_2[token_b] if (freq_table_2[token_b] != 0) else 1
                    matching_score += (1 / (( incidence_1 / num_tokens_1) * ( incidence_2 / num_tokens_2)))
        pair = {
            1: sentences_1[i],
            2: sentences_2[j],
            "matchingWordRarity": matching_score
        }
        pairs.append(pair)

sorted_pairs = sort_by_matching_score(pairs)

# ------- WRITE OUTPUT AS JSON FILE ------- # 
with open(output_path, 'w') as output:
    json.dump(sorted_pairs, output, indent = 4)

print("\n...finished writing output JSON files\n")